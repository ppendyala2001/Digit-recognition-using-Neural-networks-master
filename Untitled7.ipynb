{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'Model'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-cdbd580bc534>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mloadmat\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mModel\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mneural_network\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mRandInitialize\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0minitialise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mPrediction\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'Model'"
     ]
    }
   ],
   "source": [
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "from Model import neural_network\n",
    "from RandInitialize import initialise\n",
    "from Prediction import predict\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "\n",
    "# Loading mat file\n",
    "data = loadmat('mnist-original.mat')\n",
    "\n",
    "# Extracting features from mat file\n",
    "X = data['data']\n",
    "X = X.transpose()\n",
    "\n",
    "# Normalizing the data\n",
    "X = X / 255\n",
    "\n",
    "# Extracting labels from mat file\n",
    "y = data['label']\n",
    "y = y.flatten()\n",
    "\n",
    "# Splitting data into training set with 60,000 examples\n",
    "X_train = X[:60000, :]\n",
    "y_train = y[:60000]\n",
    "\n",
    "# Splitting data into testing set with 10,000 examples\n",
    "X_test = X[60000:, :]\n",
    "y_test = y[60000:]\n",
    "\n",
    "m = X.shape[0]\n",
    "input_layer_size = 784 # Images are of (28 X 28) px so there will be 784 features\n",
    "hidden_layer_size = 100\n",
    "num_labels = 10 # There are 10 classes [0, 9]\n",
    "\n",
    "# Randomly initialising Thetas\n",
    "initial_Theta1 = initialise(hidden_layer_size, input_layer_size)\n",
    "initial_Theta2 = initialise(num_labels, hidden_layer_size)\n",
    "\n",
    "# Unrolling parameters into a single column vector\n",
    "initial_nn_params = np.concatenate((initial_Theta1.flatten(), initial_Theta2.flatten()))\n",
    "maxiter = 100\n",
    "lambda_reg = 0.1 # To avoid overfitting\n",
    "myargs = (input_layer_size, hidden_layer_size, num_labels, X_train, y_train, lambda_reg)\n",
    "\n",
    "# Calling minimize function to minimize cost function and to train weights\n",
    "results = minimize(neural_network, x0=initial_nn_params, args=myargs,\n",
    "\t\toptions={'disp': True, 'maxiter': maxiter}, method=\"L-BFGS-B\", jac=True)\n",
    "\n",
    "nn_params = results[\"x\"] # Trained Theta is extracted\n",
    "\n",
    "# Weights are split back to Theta1, Theta2\n",
    "Theta1 = np.reshape(nn_params[:hidden_layer_size * (input_layer_size + 1)], (\n",
    "\t\t\t\t\t\t\thidden_layer_size, input_layer_size + 1)) # shape = (100, 785)\n",
    "Theta2 = np.reshape(nn_params[hidden_layer_size * (input_layer_size + 1):],\n",
    "\t\t\t\t\t(num_labels, hidden_layer_size + 1)) # shape = (10, 101)\n",
    "\n",
    "# Checking test set accuracy of our model\n",
    "pred = predict(Theta1, Theta2, X_test)\n",
    "print('Test Set Accuracy: {:f}'.format((np.mean(pred == y_test) * 100)))\n",
    "\n",
    "# Checking train set accuracy of our model\n",
    "pred = predict(Theta1, Theta2, X_train)\n",
    "print('Training Set Accuracy: {:f}'.format((np.mean(pred == y_train) * 100)))\n",
    "\n",
    "# Evaluating precision of our model\n",
    "true_positive = 0\n",
    "for i in range(len(pred)):\n",
    "\tif pred[i] == y_train[i]:\n",
    "\t\ttrue_positive += 1\n",
    "false_positive = len(y_train) - true_positive\n",
    "print('Precision =', true_positive/(true_positive + false_positive))\n",
    "\n",
    "# Saving Thetas in .txt file\n",
    "np.savetxt('Theta1.txt', Theta1, delimiter=' ')\n",
    "np.savetxt('Theta2.txt', Theta2, delimiter=' ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
